# omniswap config for vllm-omni backends
#
# All models share GPUs and swap on demand.
# Only one model is loaded at a time (default group: swap=true, exclusive=true).

healthCheckTimeout: 600
logLevel: info
logToStdout: both

macros:
  "sc": "/app/stage-configs"
  "vllm-base": >
    --omni
    --host 127.0.0.1
    --enforce-eager
    --trust-remote-code

models:
  # ---------------------------------------------------------------------------
  # Qwen3-TTS — text-to-speech (single GPU)
  # ---------------------------------------------------------------------------

  # 1.7B VoiceDesign — voice design from text descriptions
  "qwen3-tts-1.7b-voicedesign":
    cmd: >
      vllm serve Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign
      --port ${PORT} ${vllm-base}
      --gpu-memory-utilization 0.9
      --stage-configs-path ${sc}/qwen3_tts.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign"

  # 1.7B CustomVoice — instruction-controlled style with 9 premium timbres
  "qwen3-tts-1.7b-customvoice":
    cmd: >
      vllm serve Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice
      --port ${PORT} ${vllm-base}
      --gpu-memory-utilization 0.9
      --stage-configs-path ${sc}/qwen3_tts.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"
    aliases:
      - "tts-1-hd"

  # 1.7B Base — 3-second voice clone from audio input
  "qwen3-tts-1.7b-base":
    cmd: >
      vllm serve Qwen/Qwen3-TTS-12Hz-1.7B-Base
      --port ${PORT} ${vllm-base}
      --gpu-memory-utilization 0.9
      --stage-configs-path ${sc}/qwen3_tts.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-TTS-12Hz-1.7B-Base"

  # 0.6B CustomVoice — 9 premium timbres, faster/lighter
  "qwen3-tts-0.6b-customvoice":
    cmd: >
      vllm serve Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
      --port ${PORT} ${vllm-base}
      --gpu-memory-utilization 0.9
      --stage-configs-path ${sc}/qwen3_tts.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice"
    aliases:
      - "tts-1"

  # 0.6B Base — 3-second voice clone, fastest
  "qwen3-tts-0.6b-base":
    cmd: >
      vllm serve Qwen/Qwen3-TTS-12Hz-0.6B-Base
      --port ${PORT} ${vllm-base}
      --gpu-memory-utilization 0.9
      --stage-configs-path ${sc}/qwen3_tts.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"

  # ---------------------------------------------------------------------------
  # Qwen2.5-Omni — multimodal chat with audio I/O (2 GPUs)
  # ---------------------------------------------------------------------------

  "qwen2.5-omni-7b":
    cmd: >
      vllm serve Qwen/Qwen2.5-Omni-7B
      --port ${PORT} ${vllm-base}
      --stage-configs-path ${sc}/qwen2_5_omni.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen2.5-Omni-7B"

  # ---------------------------------------------------------------------------
  # Qwen3-Omni — multimodal chat with audio I/O, MoE (2 GPUs)
  # ---------------------------------------------------------------------------

  "qwen3-omni-8b":
    cmd: >
      vllm serve Qwen/Qwen3-Omni-8B
      --port ${PORT} ${vllm-base}
      --stage-configs-path ${sc}/qwen3_omni_moe.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "Qwen/Qwen3-Omni-8B"

  # ---------------------------------------------------------------------------
  # Bagel — text/image-to-image diffusion (2 GPUs)
  # ---------------------------------------------------------------------------

  "bagel-7b-mot":
    cmd: >
      vllm serve ByteDance-Seed/Bagel-7B-MoT
      --port ${PORT} ${vllm-base}
      --stage-configs-path ${sc}/bagel.yaml
    proxy: "http://127.0.0.1:${PORT}"
    useModelName: "ByteDance-Seed/Bagel-7B-MoT"
